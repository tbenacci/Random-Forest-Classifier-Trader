---
title: "data_prep"
author: "Thomas Benacci"
date: "2025-11-12"
output: html_document
---

### Libraries
```{r, include=FALSE}
library(tidyverse)
library(lubridate)
library(zoo)
library(pbapply)
library(data.table)
```

### Raw Data
```{r}
# funda cols
funda_cols <- c(
    # Size
    "cshoq",  # Common shares outstanding
    "prccq",  # Price per share (quarter-end)
    # Value
    "ceqq",   # Common equity
    "seqq", 
    # Quality
    "epspxq",  # Earnings per share (EPS)
    "dlttq",   # Long-term debt
    "atq",     # Total assets
    # Profitability
    "niq",    # Net income
    "saleq",  # Sales (Revenue)
    "oibdpq", # Operating income before depreciation
    "cogsq",  # Cost of goods sold
    "piq",    # Pre-tax income
    "oancfy", # Operating cash flow
    "icaptq", # Invested capital 
    "revtq",  # Total revenue
    # Growth
    "actq",   # Current assets
    "lctq",   # Current liabilities
    "invtq",   # Inventory
    "dlttq",  # Long-term debt (used again)
    "dlcq",  # Current debt
    "cheq",   # Cash & equivalents
    # Turnover Ratios
    "rectq",  # Receivables
    "apq",   # Accounts payable
    # Sector classification
    "gsector", 
    "mibq", 
    "pstkq", 
    "capxy",
    "dvpq", 
    "ivncfy"
    )
# quarterly fundamental data
funda <- fread(
  "F:/CRSP/fundamental_quarterly_196103_202504.csv", 
  data.table = F
  ) %>%
    mutate(
      rdq = ifelse(is.na(rdq), as.Date(datadate) %m+% months(2), rdq),
      rdq = as.Date(rdq), # make sure is as.Date dtype
      bom = ceiling_date(rdq, "month"), # assume data is available the first day of the next month
      ) %>%
    select(
      rdq, bom, LPERMNO, tic, all_of(funda_cols)
      ) %>%
    group_by(LPERMNO) %>%
    arrange(bom) %>%
    fill(
      atq, saleq, epspxq, 
      .direction = "down"
      ) %>%
    mutate(
      char_epsg = epspxq / lag(epspxq, 1) - 1, # earnings per share growth
      char_ag = atq / lag(atq, 1) - 1, # asset growth
      ttm_sg = saleq / lag(saleq, 1) - 1, # sale growth
      ) %>%
    ungroup()
# daily price data
daily <- fread(
  "F:/CRSP/daily_prices_20000101_20250504.csv", 
  data.table = F
  ) %>%
  mutate(
    datadate = as.Date(datadate),
    bom = floor_date(datadate, "month") # the first day of the current month, used to match with funda bom 
    ) %>%
  # American exchanges
  filter(exchg %in% c(11,12,14)) %>%
  # price columns necessary for feature creation
  select(datadate, LPERMNO, tic, bom, ajexdi, cshoc, cshtrd, prccd, prcod)
# daily SPY data (proxy for the market portfolio)
spy_daily <- fread("SPY_history.csv", data.table = F) %>%
  mutate(
    # calculate adjustment factor for SPY
    ajexdi = Close / `Adj. Close`,
    # rename Adj Close col
    spy_aprccd = `Adj. Close`,
    spy_aprcod = Open / ajexdi, 
    # calculate SPY one day return
    spy_CR1d = spy_aprccd / lag(spy_aprccd, 1) - 1, # close
    spy_OR1d = spy_aprcod / lag(spy_aprcod, 1) - 1, # open
    # calculate the 1-day lag SPY daily return
    spy_lagR1d = lag(spy_aprccd, 1) / lag(spy_aprccd, 2) - 1,
    # index momentum variables
    spy_mom12mo = spy_aprccd / lag(spy_aprccd, 252) - 1,
    spy_mom6mo = spy_aprccd / lag(spy_aprccd, 126) - 1,
    spy_mom1mo = spy_aprccd / lag(spy_aprccd, 21) - 1,
    ) %>%
  # rename Date to datadate (for merging with CRSP/Compustat)
  rename(datadate = Date) %>%
  # select the necessary columns
  select(datadate, spy_aprccd, spy_aprcod, spy_CR1d, spy_OR1d, spy_mom12mo, spy_mom6mo, spy_mom1mo)
gc()
```
### SPY Holiday Data
```{r}
spy_daily <- spy_daily %>%
  mutate(datadate = as.Date(datadate),
         day_int = case_when(
           weekdays(datadate) == "Monday"    ~ 3,
           weekdays(datadate) == "Tuesday"   ~ 4,
           weekdays(datadate) == "Wednesday" ~ 5,
           weekdays(datadate) == "Thursday"  ~ 1,
           weekdays(datadate) == "Friday"    ~ 2,
           TRUE ~ NA # in case of weekends
         ))
#
spy_daily <- spy_daily %>%
  mutate(
    orig_week_id = as.integer(
      paste0(format(datadate - (as.numeric(format(datadate, "%u")) - 4), "%Y"),
             format(datadate - (as.numeric(format(datadate, "%u")) - 4), "%V"))
    )
  )

# Step 2: get unique week_ids in order
week_order <- sort(unique(spy_daily$orig_week_id))

# Step 3: create a mapping from each week_id to the next week_id
next_week_map <- setNames(c(week_order[-1], NA), week_order)

# Step 4: assign week_id
spy_daily <- spy_daily %>%
  mutate(
    week_id = ifelse(
      day_int %in% c(1,2),
      next_week_map[as.character(orig_week_id)],
      orig_week_id
    )
  ) %>%
  select(-orig_week_id)
#
spy_daily <- spy_daily %>%
  group_by(week_id) %>%
  mutate(
    spy_buy_price  = spy_aprcod[which.min(day_int)],
    spy_sell_price = spy_aprccd[which.max(day_int)]
  ) %>%
  ungroup() %>%
  mutate(
    spy_holiday_return = spy_sell_price / spy_buy_price - 1
  )
#
spy_daily %>%
  select(datadate, spy_aprcod, spy_aprccd, day_int, week_id, spy_buy_price, spy_sell_price, spy_holiday_return)
#
spy_daily %>%
  group_by(week_id) %>%
  summarise(rows_per_week = n(), .groups = "drop") %>%
  ggplot(aes(x = rows_per_week)) +
  geom_histogram(binwidth = 1, fill = "green", color = "black") +
  geom_text(stat = "count", aes(label = ..count..), vjust = -0.5) +
  labs(
    title = "Histogram of Rows per Week",
    x = "Number of Rows in Week",
    y = "Count of Weeks"
  ) +
  theme_minimal()
# de-select these unecessary columns
spy_daily <- spy_daily %>%
  select(-day_int, -week_id)
```

### STOCKS Holiday data (added 11/10/2025)
```{r}
daily <- daily %>%
  mutate(datadate = as.Date(datadate),
         day_int = case_when(
           weekdays(datadate) == "Monday"    ~ 3,
           weekdays(datadate) == "Tuesday"   ~ 4,
           weekdays(datadate) == "Wednesday" ~ 5,
           weekdays(datadate) == "Thursday"  ~ 1,
           weekdays(datadate) == "Friday"    ~ 2,
           TRUE ~ NA_real_ # in case of weekends
         ))
#
daily <- daily %>%
  mutate(
    orig_week_id = as.integer(
      paste0(format(datadate - (as.numeric(format(datadate, "%u")) - 4), "%Y"),
             format(datadate - (as.numeric(format(datadate, "%u")) - 4), "%V"))
    )
  )
#
week_order <- sort(unique(daily$orig_week_id))
#
next_week_map <- setNames(c(week_order[-1], NA), week_order)
#
daily <- daily %>%
  mutate(
    week_id = ifelse(
      day_int %in% c(1,2),
      next_week_map[as.character(orig_week_id)],
      orig_week_id
    )
  ) %>%
  select(-orig_week_id)
#
daily <- daily %>%
  group_by(LPERMNO) %>%
  arrange(datadate) %>%
  mutate(
    prcod = if_else(is.na(prcod), lag(prccd), prcod)
  ) %>%
  ungroup() %>%
  filter(!is.na(prccd)) %>%
  filter(!is.na(prcod)) %>%
  mutate(
    aprccd = prccd / ajexdi, # adjusted day close price (t)
    aprcod = prcod / ajexdi, # adjusted day open price (t)
  ) %>%
  group_by(LPERMNO, week_id) %>%
  mutate(
    buy_price  = aprcod[which.min(day_int)],
    sell_price = aprccd[which.max(day_int)]
  ) %>%
  ungroup()
# check to make sure it works
daily %>%
  filter(tic %in% "AAPL") %>%
  select(datadate, tic, aprcod, aprccd, day_int, week_id, buy_price, sell_price, LPERMNO) %>%
  group_by(LPERMNO) %>%
  arrange(datadate) %>%
  ungroup()
```

### Added 8/30/2025
```{r}
# compute market cap variable
daily <- daily %>%
  mutate(market_cap = cshoc * prccd)
# get top 500 LPERMNOs per date
top500 <- daily %>%
  group_by(datadate) %>%
  slice_max(order_by = market_cap, n = 500, with_ties = FALSE) %>%
  ungroup() %>%
  distinct(datadate, LPERMNO)
# assuming top500 is a data frame with column LPERMNO
top500_ids <- unique(top500$LPERMNO)
# all time data for stocks that ever made it to the top 500 in the entire data set
daily <- daily %>%
  filter(LPERMNO %in% top500_ids)
```

### Merge
```{r}
# merge daily with spy daily
daily <- merge(daily, spy_daily, all.x = TRUE)
# merge daily with funda
data <- merge(daily, funda, all.x = TRUE) 
rm(daily)
rm(funda)
data <- data %>%
  group_by(LPERMNO) %>%
  arrange(datadate) %>%
  fill(
    everything(), 
    .direction = "down"
    ) %>%
  ungroup()
```

```{r}
# Relative strength index function
calc_rsi <- function(prices, n = 14){
  deltas <- diff(prices)
  gains <- ifelse(deltas > 0, deltas, 0)
  losses <- ifelse(deltas < 0, -deltas, 0)
  avg_gain <- rollapply(gains, width = n, FUN = mean, align = "right", fill = NA)
  avg_loss <- rollapply(losses, width = n, FUN = mean, align = "right", fill = NA)
  rs <- avg_gain / avg_loss
  rsi <- 100 - (100 / (1 + rs))
  c(NA, rsi) # Pad to match original length after `diff()`
}
```

### Features
Assume all data is available at the end of the day $t$ for feature generation. Trading is done at the open of day $t+1$. The portfolio created on $t+1$ is held until open at $t+dt$ when it is turned over. 
```{r}
# t is an index of daily frequency
data <- data %>%
  # filter(tic %in% "AAPL") %>% # un-comment for quick test on just AAPL
  group_by(LPERMNO) %>%
  # datadate from daily data
  arrange(datadate) %>%
  mutate(
    ## Fundamental
    # Size
    char_mcap = cshoq * prccq, # market capitalization of equity
    # Quality
    char_bm = ceqq / char_mcap, # book-to-market ratio
    char_fl = (dlttq + dlcq) / seqq, # financial leverage
    # Profitability
    char_roic = niq / icaptq, # return on invested capital 
    # Profitability features that still require TTM
    ttm_ni_mcap = niq / char_mcap, 
    ev = char_mcap + coalesce(dlttq, 0) + coalesce(dlcq, 0) + coalesce(pstkq, 0) + coalesce(mibq, 0) - coalesce(cheq, 0), # enterprise value
    ttm_sale_ev = saleq / ev,
    ttm_cash_mcap = oancfy / char_mcap,
    ttm_fcf_mcap = (oancfy - coalesce(capxy, 0) + coalesce(dlttq, 0) + coalesce(dlcq, 0) - coalesce(dvpq, 0)) / char_mcap,
    ttm_fcf_ev = (oancfy - coalesce(capxy, 0) + coalesce(dlttq, 0) + coalesce(dlcq, 0) - coalesce(dvpq, 0)) / ev,
    ttm_dy = dvpq / char_mcap,
    ttm_om = oibdpq / saleq,
    ttm_pm = niq / saleq, 
    # Growth 
    # Growth features that still require TTM
    ttm_cfia = ivncfy / ev, 
    ## TTM mutation
    # I chose to use the average of the previous 252 trading days to calculate TTM
    # https://www.investopedia.com/terms/t/ttm.asp
    across(starts_with("ttm_"), 
           ~ rollapply(.x, width = 252, 
                       FUN = mean, 
                       align = "right", 
                       fill = NA),
            .names = "char_{.col}"
           ),
    ## Technical
    # Momentum
    CR1d = aprccd / lag(aprccd, 1) - 1, # close 1 day return (t-1, t)
    OR1d = aprcod / lag(aprcod, 1) - 1, # open 1 day return (t-1, t)
    char_mom12m = aprccd / lag(aprccd, 252) - 1, # momentum 12 mo
    char_mom6m = aprccd / lag(aprccd, 126) - 1, # momentum 6 mo
    char_mom1m = aprccd / lag(aprccd, 21) - 1, # momentum 1 mo
    char_relmom12m = char_mom12m / spy_mom12mo - 1, # relative momentum 12 mo
    char_relmom6m = char_mom6m / spy_mom6mo - 1, # relative momentum 6 mo
    char_relmom1m = char_mom1m / spy_mom1mo - 1, # relative momentum 1 mo
    # Moving averages
    char_pma200 = aprccd / rollapply(aprccd, width = 200, FUN = mean, align = "right", fill = NA), # log(price/moving average 200D)
    char_pma100 = aprccd / rollapply(aprccd, width = 100, FUN = mean, align = "right", fill = NA), # log(price/moving average 100D)
    char_pma50 = aprccd / rollapply(aprccd, width = 50, FUN = mean, align = "right", fill = NA), # log(price/moving average 50D)
    # Risk
    mean_R1d_12m = rollapply(CR1d, width = 252, FUN = mean, align = "right", fill = NA),
    mean_spyR1d_12m = rollapply(spy_CR1d, width = 252, FUN = mean, align = "right", fill = NA),
    # Rolling covariance
    cov_12m = rollapply(
      (CR1d - mean_R1d_12m) * (spy_CR1d - mean_spyR1d_12m),
      width = 252,
      FUN = mean,
      align = "right",
      fill = NA
      ),
    # Rolling variance of market return
    var_spy_12m = rollapply(
      spy_CR1d,
      width = 252,
      FUN = var,
      align = "right",
      fill = NA
      ),
    # Beta = Cov(R_stock, R_market) / Var(R_market)
    char_beta_12m = cov_12m / var_spy_12m,
    # Beta 12M
    char_vol_12m = rollapply(CR1d, width = 252, FUN = sd, align = "right", fill = NA), # Volatility 12M
    char_vol_6m = rollapply(CR1d, width = 126, FUN = sd, align = "right", fill = NA), # Volatility 6M
    char_vol_1m = rollapply(CR1d, width = 21, FUN = sd, align = "right", fill = NA), # Volatility 1M
    # Short-term reversal
    char_rsi_14d = calc_rsi(aprccd, 14), # Relative strength index 14D
    char_rsi_9d = calc_rsi(aprccd, 9), # Relative strength index 9D
    char_rsi_3d = calc_rsi(aprccd, 3), # Relative strength index 3D
    ma20 = rollapply(aprccd, width = 20, FUN = mean, align = "right", fill = NA),
    sd20 = rollapply(aprccd, width = 20, FUN = sd, align = "right", fill = NA),
    bb_upper = ma20 + 2 * sd20,
    bb_lower = ma20 - 2 * sd20,
    char_log_pbbup = if_else(bb_upper > 0 & !is.na(bb_upper), log(aprccd / bb_upper), NA_real_),
    char_log_pbblow = if_else(bb_lower > 0 & !is.na(bb_lower), log(aprccd / bb_lower), NA_real_),
    char_lagR1d = lag(aprccd, 1) / lag(aprccd, 2) - 1, # Lagged return (R(t-1), R(t-2))
    char_dolvol = cshtrd*prccd # Trading Volume
    ) %>%
  ungroup()
```

### Holiday-sensitive returns
```{r}
data <- data %>%
  mutate(holiday_return = sell_price / buy_price - 1)
```

```{r}
data <- data %>%
    filter(datadate >= "2004-01-01")
# replace inf with NA
data <- data %>%
  mutate(across(starts_with("char_"), ~ na_if(., Inf))) %>%
  mutate(across(starts_with("char_"), ~ na_if(., -Inf)))
#
colnames(data %>% select(starts_with("char_")))
summary(data %>% select(starts_with("char_")))
```

### Lag the characteristics
```{r}
data_lag <- data %>%
  group_by(LPERMNO) %>%
  arrange(datadate, .by_group = TRUE) %>%
  mutate(across(starts_with("char_"), ~ dplyr::lag(.x, 1))) %>%
  ungroup()
```

### Standardize
```{r}
# standardization function
standardize_func <- function(x){
  mean_x <- mean(x, na.rm = TRUE)
  sd_x <- sd(x, na.rm = TRUE)
  if (sd_x == 0 || is.na(sd_x)){
    return(rep(NA_real_, length(x))) # If there's no variation (sd = 0), return all NAs
  } else {
    return((x - mean_x) / sd_x)
  }
}
# apply the standardization function
data_stand <- data_lag %>%
  ##########################
  # 9/1/2025 data
  ##########################
  mutate(
    weekday = wday(datadate)
    ) %>%
  filter(weekday %in% 5) %>%
  group_by(datadate) %>%
  # consider top 500 stocks each day by mcap
  arrange(desc(char_mcap), .by_group = TRUE) %>%
  slice_head(n = 500) %>%
  ##########################
  # 9/1/2025 data
  ##########################
  group_by(datadate) %>%
  mutate(across(
    .cols = starts_with("char_"),
    .fns = standardize_func
  )) %>%
  ungroup()
#
summary(data_stand %>% select(starts_with("char_")))
```

### Fill Down and Make Target
```{r}
filt_data <- data_stand %>% 
  # select necessary rows
  select(datadate, LPERMNO, tic, holiday_return, spy_holiday_return, starts_with("char_")) %>%
  group_by(LPERMNO) %>%
  arrange(datadate) %>%
  # fill in missing elements
  fill(
    everything(), 
    .direction = "down"
    ) %>%
  ungroup() %>%
  # replace NA values with 0
  mutate(across(starts_with("char_"), ~replace_na(.x, 0))) %>%
  # create the target variable
  group_by(LPERMNO) %>%
  arrange(datadate) %>%
  mutate(
    target = ifelse(holiday_return > spy_holiday_return, 1, 0), 
    weekday = wday(datadate), 
    year = year(datadate)
    ) %>%
  ungroup() %>%
  drop_na(target) %>%
  # consider top 500 stocks each day by mcap
  group_by(datadate) %>% 
  arrange(desc(char_mcap), .by_group = TRUE) %>%
  slice_head(n = 500) %>%
  ungroup()
#
filt_data <- filt_data %>%
  group_by(datadate) %>%
  # since the characteristics are lagged 1 day (Wednesday, usually), we select Thursday dates, as they are the BUY dates (usually)
  filter(n() == 500, weekday %in% 5) %>%
  ungroup()
#
summary(filt_data)
```
### Save data
```{r}
# save data as csv for input into prediction/signal generating program of choice
#write_csv(filt_data, "feature_data.csv")
write_csv(filt_data, "feature_data_holiday_20251112.csv")
```


